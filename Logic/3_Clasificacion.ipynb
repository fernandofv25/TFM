{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e083279a",
   "metadata": {},
   "source": [
    "# CLASIFIACION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c4505ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "import traceback\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras import layers, models\n",
    "from keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a62404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre el archivo JSON y lee su contenido\n",
    "#with open('datos.json', 'r') as f:\n",
    "#    datos_json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61873738",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S001R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S002R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S003R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S004R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S005R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S006R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S007R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S008R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S009R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R008.json'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S010R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S011R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S012R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S013R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S014R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S015R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S016R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S017R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S018R014.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S019R014.json'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R003.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R004.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R005.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R006.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R007.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R008.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R009.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R010.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R011.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R012.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R013.json'.\n",
      "Datos cargados exitosamente desde '../Data/Preprocessed/S020R014.json'.\n",
      "/nDatos cargados exitosamente desde la carpeta preprocessed.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Lista para almacenar los datos de todos los archivos\n",
    "data = []\n",
    "\n",
    "try:\n",
    "    # Obtener la lista de archivos en la carpeta preprocessed\n",
    "    carpeta_preprocessed = '../Data/Preprocessed/'\n",
    "    archivos_preprocessed = os.listdir(carpeta_preprocessed)\n",
    "    \n",
    "    # Iterar sobre cada archivo\n",
    "    for archivo in archivos_preprocessed:\n",
    "        # Construir la ruta completa del archivo\n",
    "        ruta_archivo = os.path.join(carpeta_preprocessed, archivo)\n",
    "        \n",
    "        # Leer el contenido del archivo JSON\n",
    "        with open(ruta_archivo, 'r') as f:\n",
    "            datos_archivo = json.load(f)\n",
    "        \n",
    "        # Agregar los datos del archivo a la lista de datos totales\n",
    "        data.append(datos_archivo)\n",
    "        \n",
    "        print(f\"Datos cargados exitosamente desde '{ruta_archivo}'.\")\n",
    "    \n",
    "    print(\"/nDatos cargados exitosamente desde la carpeta preprocessed.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error al cargar los datos desde {ruta_archivo}.\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf6574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240\n",
      "64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "float"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(len(data[0]))\n",
    "type(data[0][0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b913a258",
   "metadata": {},
   "source": [
    "## Preparación de datos\n",
    "\n",
    "Para preparar nuestros datos, primero debemos estructurarlos en secuencias que representen las diversas actividades realizadas durante las sesiones de EEG. Cada paciente sigue un protocolo predefinido, ejecutando una serie de tareas en un orden específico que abarca desde la línea de base hasta acciones motoras e imaginadas. Este orden se presenta de la siguiente manera:\n",
    "\n",
    "1. Baseline, eyes open\n",
    "2. Baseline, eyes closed\n",
    "3. Task 1 (open and close left or right fist)\n",
    "4. Task 2 (imagine opening and closing left or right fist)\n",
    "5. Task 3 (open and close both fists or both feet)\n",
    "6. Task 4 (imagine opening and closing both fists or both feet)\n",
    "7. Task 1\n",
    "8. Task 2\n",
    "9. Task 3\n",
    "10. Task 4\n",
    "11. Task 1\n",
    "12. Task 2\n",
    "13. Task 3\n",
    "14. Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f25d2fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_secuencia(x):\n",
    "    secuencia = [0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n",
    "    pacientes = x// 12\n",
    "    res = []\n",
    "    for e in range(pacientes):\n",
    "        res.extend(secuencia)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e0bd817a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3, 0, 1, 2, 3]\n",
      "Total registros: 240\n",
      "Total registros: 240\n",
      "Total de registros por canal: 20000\n"
     ]
    }
   ],
   "source": [
    "data_clasificacion = generar_secuencia(len(data))\n",
    "print(data_clasificacion)\n",
    "print(f\"Total registros: {len(data_clasificacion)}\")\n",
    "print(f\"Total registros: {len(data)}\")\n",
    "print(f\"Total de registros por canal: {len(data[0][0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a036ed1",
   "metadata": {},
   "source": [
    "Ejemplo de datos para prueba:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10c707d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data_prueba = [\n",
    "    [[47, 69, 12, 85, 52],[38, 19, 94, 25, 10]],\n",
    "    [[85, 60, 17, 89, 28],[18, 17, 77, 83, 23]],\n",
    "    [[79, 5, 38, 97, 26],[7, 53, 70, 82, 44]],\n",
    "    [[73, 86, 99, 54, 7],[66, 42, 65, 63, 56]],\n",
    "    [[20, 92, 78, 82, 63],[97, 8, 51, 13, 19]],\n",
    "    [[51, 99, 24, 67, 97],[13, 40, 4, 36, 41]],\n",
    "    [[28, 62, 57, 54, 100],[31, 81, 52, 46, 31]],\n",
    "    [[26, 65, 16, 78, 51],[65, 44, 41, 41, 70]],\n",
    "    [[97, 77, 93, 30, 61],[23, 85, 79, 60, 44]],\n",
    "    [[90, 28, 14, 15, 99],[97, 26, 30, 53, 92]],\n",
    "]\n",
    "actividades_prueba = [1, 2, 3, 4, 5, 6, 4, 2, 2, 5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc2c2903",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -57,  -49,  -55, ...,    0,    0,    0],\n",
       "       [ -13,  -11,  -17, ...,    0,    0,    0],\n",
       "       [ -15,  -10,  -16, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [ -56,  -70,  -77, ...,    0,    0,    0],\n",
       "       [-124, -149, -153, ...,    0,    0,    0],\n",
       "       [ -28,  -40,  -37, ...,    0,    0,    0]], dtype=int16)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datitos = np.array(data[0], dtype=np.int16)\n",
    "datitos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adbb4f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encontrar el máximo número de valores en un dato\n",
    "max_len = max(len(registro) for sublist in data for registro in sublist)\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d13230b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a numpy arrays\n",
    "#eeg_data = np.array(data).astype(np.float32)\n",
    "eeg_data = np.array(data, dtype=np.float32)\n",
    "actividades = np.array(data_clasificacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9520cd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAMBIAR DATOS AQUI---------\n",
    "# Dividir datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(eeg_data, actividades, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4887d5",
   "metadata": {},
   "source": [
    "## Creamos el modelo LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "639924bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(max_len, len(eeg_data[0][0]))),  # Input shape: número variable de registros por electrodo\n",
    "    tf.keras.layers.LSTM(64),  # Capa LSTM para aprender secuencias temporales\n",
    "    tf.keras.layers.Dense(32, activation='relu'),  # Capa densa\n",
    "    tf.keras.layers.Dense(6, activation='softmax')  # Capa de salida con activación softmax para clasificación\n",
    "])\n",
    "\n",
    "# Compila el modelo\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd41e301",
   "metadata": {},
   "source": [
    "Entrenamos el modelo, añadimos verbose, para que nos muestre el proceso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eabb08b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m162s\u001b[0m 2s/step - accuracy: 0.1552 - loss: 1.9271 - val_accuracy: 0.3333 - val_loss: 1.6883\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.3993 - loss: 1.6420 - val_accuracy: 0.3542 - val_loss: 1.7214\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.4411 - loss: 1.5265 - val_accuracy: 0.2292 - val_loss: 1.7477\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.5373 - loss: 1.4594 - val_accuracy: 0.1667 - val_loss: 1.7892\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6055 - loss: 1.3371 - val_accuracy: 0.1875 - val_loss: 1.7714\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 997ms/step - accuracy: 0.5481 - loss: 1.3303 - val_accuracy: 0.3125 - val_loss: 1.7039\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 994ms/step - accuracy: 0.6004 - loss: 1.2799 - val_accuracy: 0.2083 - val_loss: 1.7148\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6897 - loss: 1.1794 - val_accuracy: 0.2708 - val_loss: 1.6384\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.6526 - loss: 1.1376 - val_accuracy: 0.3333 - val_loss: 1.5921\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 1s/step - accuracy: 0.7065 - loss: 1.0962 - val_accuracy: 0.2500 - val_loss: 1.6240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x22ae6049d60>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entrena el modelo\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47861c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 243ms/step - accuracy: 0.2500 - loss: 1.6257\n",
      "Accuracy on test data: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Evalúa el modelo\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad29fbb3",
   "metadata": {},
   "source": [
    "## Creamos el modelo CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50770982",
   "metadata": {},
   "source": [
    "Esta red neuronal convolucional (CNN) está diseñada para procesar datos de electroencefalografía (EEG) representados en un array de tres niveles. Aquí está cómo procesa los datos:\n",
    "\n",
    "Capa de entrada (Conv1D):\n",
    "La primera capa Conv1D se utiliza para convolucionar los datos de entrada.\n",
    "Se especifica 64 como el número de filtros.\n",
    "El tamaño del kernel es 3.\n",
    "La función de activación relu se utiliza para introducir no linealidades.\n",
    "La forma de entrada se especifica como (len(eeg_data[0]), len(eeg_data[0][0])), lo que corresponde al tamaño del segundo y tercer nivel del array de datos.\n",
    "Capa de agrupación (MaxPooling1D):\n",
    "Después de la convolución, se aplica una capa de agrupación máxima (MaxPooling1D) con un tamaño de agrupación de 2.\n",
    "Esto reduce la dimensionalidad de los datos y extrae las características más importantes.\n",
    "Segunda capa de convolución (Conv1D):\n",
    "Se agrega otra capa Conv1D para extraer características más complejas.\n",
    "Esta capa también tiene 64 filtros y un tamaño de kernel de 3.\n",
    "Segunda capa de agrupación (MaxPooling1D):\n",
    "Se aplica otra capa de agrupación máxima (MaxPooling1D) con un tamaño de agrupación de 2.\n",
    "Capa de aplanado (Flatten):\n",
    "Después de las capas de convolución y agrupación, los datos se aplanan en un vector unidimensional para alimentar las capas densas posteriores.\n",
    "Capas densas (Dense):\n",
    "Se agregan dos capas densas (Dense) para realizar la clasificación final.\n",
    "La primera capa densa tiene 64 neuronas con una función de activación relu.\n",
    "La segunda capa densa tiene 6 neuronas con una función de activación softmax.\n",
    "La capa final produce una distribución de probabilidad sobre las clases, ya que se utiliza la función de activación softmax.\n",
    "En cuanto al entrenamiento y evaluación del modelo:\n",
    "\n",
    "El modelo se compila con el optimizador 'adam' y la pérdida se calcula utilizando la entropía cruzada categórica escasa ('sparse_categorical_crossentropy').\n",
    "Se entrena el modelo usando los datos de entrenamiento (X_train y y_train) durante 10 épocas, validando en los datos de prueba (X_test y y_test).\n",
    "Finalmente, se evalúa el modelo en los datos de prueba y se imprime la precisión obtenida.\n",
    "Ten en cuenta que en el código proporcionado, deberías reemplazar X_train, y_train, X_test y y_test con los datos de entrenamiento y prueba reales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b8716",
   "metadata": {},
   "source": [
    "### Modelo 1, basico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eabe3627",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Fernando\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 327ms/step - accuracy: 0.2987 - loss: 195.1993 - val_accuracy: 0.2292 - val_loss: 206.5858\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 272ms/step - accuracy: 0.4349 - loss: 105.2990 - val_accuracy: 0.2292 - val_loss: 130.3690\n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 292ms/step - accuracy: 0.6605 - loss: 29.2862 - val_accuracy: 0.1667 - val_loss: 204.5600\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 287ms/step - accuracy: 0.8153 - loss: 10.6907 - val_accuracy: 0.1458 - val_loss: 208.9408\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 295ms/step - accuracy: 0.8743 - loss: 4.3895 - val_accuracy: 0.2083 - val_loss: 144.9064\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 287ms/step - accuracy: 0.9179 - loss: 4.3706 - val_accuracy: 0.1667 - val_loss: 142.0141\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 293ms/step - accuracy: 0.9530 - loss: 0.6700 - val_accuracy: 0.1875 - val_loss: 206.3477\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 290ms/step - accuracy: 0.9499 - loss: 6.4514 - val_accuracy: 0.1875 - val_loss: 194.6048\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 296ms/step - accuracy: 0.9854 - loss: 0.4384 - val_accuracy: 0.2292 - val_loss: 175.2401\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 298ms/step - accuracy: 0.9883 - loss: 0.0206 - val_accuracy: 0.2083 - val_loss: 171.9310\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 264ms/step - accuracy: 0.2500 - loss: 1.6257\n",
      "Accuracy on test data: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Construir el modelo CNN\n",
    "model_CNN = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu', input_shape=(len(eeg_data[0]), len(eeg_data[0][0]))),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
    "    tf.keras.layers.MaxPooling1D(2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo\n",
    "model_CNN.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo\n",
    "model_CNN.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdb899d",
   "metadata": {},
   "source": [
    "### Modelo 2, mejorar modelo basico"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2563468",
   "metadata": {},
   "source": [
    "Este modelo es una Convolutional Neural Network (CNN) diseñada para procesar señales EEG (Electroencefalografía) para clasificarlas en diferentes categorías. Veamos cada parte detalladamente:\n",
    "\n",
    "**Capas Convolucionales**:\n",
    "La primera capa convolucional tiene 32 filtros de tamaño 3 y función de activación ReLU (Rectified Linear Unit). La forma de entrada se especifica en input_shape.\n",
    "Después de cada capa convolucional, hay una capa de Max Pooling con tamaño de pool 2, lo que reduce la dimensionalidad espacial de la representación.\n",
    "Este patrón se repite dos veces más, aumentando el número de filtros a 64 en la segunda capa y a 128 en la tercera capa.\n",
    "\n",
    "**Aplanamiento (Flattening)**:\n",
    "Después de las capas convolucionales y de pooling, los datos se aplanan en un vector unidimensional para poder alimentarlos a una capa densa.\n",
    "\n",
    "**Capa Densa (Fully Connected Layer)**:\n",
    "Se agrega una capa densa con 128 neuronas y función de activación ReLU.\n",
    "También se aplica Dropout con una tasa del 50% para reducir el sobreajuste. El dropout elimina aleatoriamente conexiones entre neuronas durante el entrenamiento, lo que ayuda a prevenir que ciertas neuronas se vuelvan dominantes.\n",
    "\n",
    "**Capa de Salida**:\n",
    "La última capa es una capa densa con un número de neuronas igual al número de clases en el problema (en este caso, num_classes), con una función de activación softmax. Softmax convierte las puntuaciones de cada clase en probabilidades, lo que permite interpretar la salida como la probabilidad de que una entrada pertenezca a cada clase.\n",
    "Compilación del Modelo:\n",
    "El modelo se compila usando el optimizador Adam, que es una variante del descenso de gradiente estocástico muy efectiva para problemas de optimización.\n",
    "La función de pérdida se establece como sparse_categorical_crossentropy, que es adecuada para problemas de clasificación con múltiples clases.\n",
    "La métrica para evaluar el rendimiento del modelo se establece en 'accuracy' para ver cuántas clasificaciones son correctas en \n",
    "relación con el total.\n",
    "\n",
    "**Resumen del Modelo**:\n",
    "Finalmente, se muestra un resumen del modelo que detalla la arquitectura de las capas, el número de parámetros entrenables y la forma de salida de cada capa. Esto brinda una visión general de cómo está construido el modelo y cuántos parámetros tiene en total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4949838d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_5\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_5\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,920,032</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)             │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">98,432</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                   │             <span style=\"color: #00af00; text-decoration-color: #00af00\">516</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d_10 (\u001b[38;5;33mConv1D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │       \u001b[38;5;34m1,920,032\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_10 (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_11 (\u001b[38;5;33mConv1D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │           \u001b[38;5;34m6,208\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_11 (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_12 (\u001b[38;5;33mConv1D\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m128\u001b[0m)             │          \u001b[38;5;34m24,704\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_12 (\u001b[38;5;33mMaxPooling1D\u001b[0m)      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ flatten_4 (\u001b[38;5;33mFlatten\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m768\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_14 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │          \u001b[38;5;34m98,432\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_15 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)                   │             \u001b[38;5;34m516\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,049,892</span> (7.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,049,892\u001b[0m (7.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,049,892</span> (7.82 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,049,892\u001b[0m (7.82 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def create_eeg_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    # Primera capa convolucional\n",
    "    model.add(layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Segunda capa convolucional\n",
    "    model.add(layers.Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Tercera capa convolucional\n",
    "    model.add(layers.Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Aplanar los datos para la capa densa\n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    # Capa densa con dropout para evitar el sobreajuste\n",
    "    model.add(layers.Dense(128, activation='relu'))\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    \n",
    "    # Capa de salida\n",
    "    model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Especifica la forma de entrada y el número de clases\n",
    "input_shape = (len(eeg_data[0]), len(eeg_data[0][0]))  # Por ejemplo, (longitud, número de canales)\n",
    "num_classes = 4  # Por ejemplo, 2 para clasificación binaria\n",
    "\n",
    "# Crea el modelo\n",
    "model = create_eeg_cnn_model(input_shape, num_classes)\n",
    "\n",
    "# Compila el modelo\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Muestra un resumen del modelo\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "94ff2ca9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - accuracy: 0.9341 - loss: 6.6679 - val_accuracy: 0.2500 - val_loss: 117.6032\n",
      "Epoch 2/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - accuracy: 0.9200 - loss: 2.6564 - val_accuracy: 0.2292 - val_loss: 119.6634\n",
      "Epoch 3/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 175ms/step - accuracy: 0.8939 - loss: 2.9461 - val_accuracy: 0.2500 - val_loss: 100.2752\n",
      "Epoch 4/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 200ms/step - accuracy: 0.9062 - loss: 8.1336 - val_accuracy: 0.3125 - val_loss: 81.6131\n",
      "Epoch 5/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 186ms/step - accuracy: 0.9217 - loss: 2.4520 - val_accuracy: 0.2917 - val_loss: 75.0444\n",
      "Epoch 6/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.9518 - loss: 0.7998 - val_accuracy: 0.2708 - val_loss: 75.3077\n",
      "Epoch 7/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.9061 - loss: 1.2078 - val_accuracy: 0.2500 - val_loss: 70.0495\n",
      "Epoch 8/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 183ms/step - accuracy: 0.9314 - loss: 1.1180 - val_accuracy: 0.2917 - val_loss: 60.0786\n",
      "Epoch 9/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step - accuracy: 0.9327 - loss: 2.1834 - val_accuracy: 0.2292 - val_loss: 73.0947\n",
      "Epoch 10/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 188ms/step - accuracy: 0.9259 - loss: 1.3496 - val_accuracy: 0.2083 - val_loss: 85.1303\n",
      "Epoch 11/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 169ms/step - accuracy: 0.9224 - loss: 0.9633 - val_accuracy: 0.2500 - val_loss: 76.4735\n",
      "Epoch 12/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.9618 - loss: 2.8953 - val_accuracy: 0.3333 - val_loss: 70.1399\n",
      "Epoch 13/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 182ms/step - accuracy: 0.9214 - loss: 1.1790 - val_accuracy: 0.2292 - val_loss: 69.3885\n",
      "Epoch 14/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 180ms/step - accuracy: 0.9397 - loss: 0.4890 - val_accuracy: 0.2083 - val_loss: 70.2438\n",
      "Epoch 15/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 168ms/step - accuracy: 0.9379 - loss: 0.4668 - val_accuracy: 0.2500 - val_loss: 68.8707\n",
      "Epoch 16/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 179ms/step - accuracy: 0.9714 - loss: 0.2153 - val_accuracy: 0.2708 - val_loss: 66.7645\n",
      "Epoch 17/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 181ms/step - accuracy: 0.9262 - loss: 3.0845 - val_accuracy: 0.2500 - val_loss: 85.4000\n",
      "Epoch 18/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 196ms/step - accuracy: 0.9858 - loss: 0.0835 - val_accuracy: 0.2083 - val_loss: 107.7853\n",
      "Epoch 19/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 238ms/step - accuracy: 0.9702 - loss: 1.2003 - val_accuracy: 0.2292 - val_loss: 102.0218\n",
      "Epoch 20/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 266ms/step - accuracy: 0.9644 - loss: 0.1301 - val_accuracy: 0.2292 - val_loss: 99.8058\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.2049 - loss: 106.8626 \n",
      "Loss en el conjunto de validación: 99.80584716796875\n",
      "Precisión en el conjunto de validación: 0.2291666716337204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y validación\n",
    "X_train, X_val, y_train, y_val = train_test_split(eeg_data, actividades, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocesamiento opcional (por ejemplo, normalización)\n",
    "# Aquí podrías aplicar cualquier tipo de preprocesamiento necesario, como normalización de los datos de EEG\n",
    "\n",
    "# Entrenar el modelo\n",
    "history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
    "\n",
    "# Evaluar el modelo en el conjunto de validación\n",
    "loss, accuracy = model.evaluate(X_val, y_val)\n",
    "print(f'Loss en el conjunto de validación: {loss}')\n",
    "print(f'Precisión en el conjunto de validación: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c795b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start...\n"
     ]
    }
   ],
   "source": [
    "# Definir la función para construir el modelo CNN\n",
    "def create_model(filters, kernel_size, pool_size, dense_units, learning_rate):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters, kernel_size, activation='relu', input_shape=(len(eeg_data[0]), len(eeg_data[0][0]))),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size),\n",
    "        tf.keras.layers.Conv1D(filters, kernel_size, activation='relu'),\n",
    "        tf.keras.layers.MaxPooling1D(pool_size),\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(dense_units, activation='relu'),\n",
    "        tf.keras.layers.Dense(6, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    # Compilar el modelo\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Crear el modelo KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model, verbose=1)\n",
    "\n",
    "# Definir los parámetros para la búsqueda de hiperparámetros\n",
    "param_grid2 = {\n",
    "    'filters': [32, 64],\n",
    "    'kernel_size': [3, 5],\n",
    "    'pool_size': [2, 3],\n",
    "    'dense_units': [64, 128],\n",
    "    'learning_rate': [0.001, 0.0001]\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'filters': [64]\n",
    "}\n",
    "\n",
    "# Realizar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "print(\"Start...\")\n",
    "grid_result = grid.fit(X_train, y_train)\n",
    "\n",
    "# Mostrar los resultados\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bd2946f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 379ms/step - accuracy: 0.1964 - loss: 137.4720 - val_accuracy: 0.1667 - val_loss: 104.4768\n",
      "Epoch 2/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 296ms/step - accuracy: 0.3082 - loss: 159.4111 - val_accuracy: 0.2917 - val_loss: 72.3042\n",
      "Epoch 3/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 350ms/step - accuracy: 0.4561 - loss: 147.1797 - val_accuracy: 0.2917 - val_loss: 116.5918\n",
      "Epoch 4/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 296ms/step - accuracy: 0.4576 - loss: 97.7726 - val_accuracy: 0.1667 - val_loss: 161.3704\n",
      "Epoch 5/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 298ms/step - accuracy: 0.4446 - loss: 158.8485 - val_accuracy: 0.3750 - val_loss: 99.3134\n",
      "Epoch 6/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 331ms/step - accuracy: 0.4801 - loss: 88.4879 - val_accuracy: 0.2083 - val_loss: 71.3148\n",
      "Epoch 7/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 313ms/step - accuracy: 0.4634 - loss: 68.6324 - val_accuracy: 0.2083 - val_loss: 71.6484\n",
      "Epoch 8/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 310ms/step - accuracy: 0.5313 - loss: 36.2485 - val_accuracy: 0.3125 - val_loss: 66.8979\n",
      "Epoch 9/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 359ms/step - accuracy: 0.6013 - loss: 41.5669 - val_accuracy: 0.3333 - val_loss: 67.9943\n",
      "Epoch 10/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 289ms/step - accuracy: 0.6479 - loss: 21.1893 - val_accuracy: 0.3750 - val_loss: 75.5760\n",
      "Epoch 11/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 291ms/step - accuracy: 0.6254 - loss: 28.1820 - val_accuracy: 0.3125 - val_loss: 70.6288\n",
      "Epoch 12/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 285ms/step - accuracy: 0.6339 - loss: 28.1145 - val_accuracy: 0.2708 - val_loss: 67.3487\n",
      "Epoch 13/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 294ms/step - accuracy: 0.6798 - loss: 13.7783 - val_accuracy: 0.2917 - val_loss: 60.7400\n",
      "Epoch 14/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 286ms/step - accuracy: 0.7251 - loss: 13.6004 - val_accuracy: 0.2500 - val_loss: 60.9663\n",
      "Epoch 15/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 287ms/step - accuracy: 0.7812 - loss: 8.6870 - val_accuracy: 0.2708 - val_loss: 58.7464\n",
      "Epoch 16/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 281ms/step - accuracy: 0.7010 - loss: 23.2223 - val_accuracy: 0.3542 - val_loss: 56.3698\n",
      "Epoch 17/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 293ms/step - accuracy: 0.7596 - loss: 8.0143 - val_accuracy: 0.3125 - val_loss: 54.2991\n",
      "Epoch 18/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 327ms/step - accuracy: 0.7462 - loss: 9.8064 - val_accuracy: 0.2500 - val_loss: 53.2507\n",
      "Epoch 19/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 326ms/step - accuracy: 0.7802 - loss: 11.6948 - val_accuracy: 0.2708 - val_loss: 55.0058\n",
      "Epoch 20/20\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 316ms/step - accuracy: 0.7815 - loss: 12.5860 - val_accuracy: 0.2708 - val_loss: 54.5312\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 80ms/step - accuracy: 0.2222 - loss: 61.4334\n",
      "Accuracy on test data: 0.2708333432674408\n"
     ]
    }
   ],
   "source": [
    "# Construir el modelo CNN con ajuste de hiperparámetros\n",
    "model = tf.keras.Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(len(eeg_data[0]), len(eeg_data[0][0]))),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Conv1D(filters=128, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),  # Capa de dropout para regularización\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(6, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compilar el modelo con optimizador Adam y tasa de aprendizaje ajustada\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar el modelo con más épocas para permitir una mejor convergencia\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_test, y_test), verbose=1)\n",
    "\n",
    "# Evaluar el modelo\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Accuracy on test data:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a3647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a1bc7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2852cd58",
   "metadata": {},
   "source": [
    "# Discusión de resultados\n",
    "Si la probabilidad de adivinar al azar una clasificación correcta entre 6 clases es del 16.7% (0.167), entonces podríamos considerar que el modelo está funcionando mejor que simplemente adivinar al azar. Dado que el modelo tiene una precisión del 25% en los datos de prueba, está superando la predicción aleatoria.\n",
    "\n",
    "Sin embargo, es importante tener en cuenta que una precisión del 25% aún es relativamente baja, lo que sugiere que el modelo aún tiene margen de mejora. Esto puede ser especialmente difícil con datos de EEG, que son señales caóticas y difíciles de predecir.\n",
    "\n",
    "Para mejorar la precisión del modelo, podrías intentar ajustar hiperparámetros del modelo, como el número de capas y neuronas, el tamaño de los filtros en las capas convolucionales, la tasa de aprendizaje del optimizador, etc. También podrías explorar técnicas de preprocesamiento de datos más avanzadas y considerar la posibilidad de utilizar arquitecturas de redes neuronales más complejas o incluso técnicas de aprendizaje profundo como redes neuronales recurrentes (RNN) si la naturaleza secuencial de los datos de EEG lo justifica. Además, recuerda que la calidad y cantidad de los datos de entrenamiento también juega un papel importante en el rendimiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368ac079",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Import useful packages\n",
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "# Hide the Configuration and Warnings\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from Models.DatasetAPI.DataLoader import DatasetLoader\n",
    "from Models.Network.CNN import CNN\n",
    "from Models.Loss_Function.Loss import loss\n",
    "from Models.Evaluation_Metrics.Metrics import evaluation\n",
    "\n",
    "# Model Name\n",
    "Model = 'Convolutional_Neural_Network'\n",
    "\n",
    "# Clear all the stack and use GPU resources as much as possible\n",
    "tf.reset_default_graph()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "# Your Dataset Location, for example EEG-Motor-Movement-Imagery-Dataset\n",
    "# The CSV file should be named as training_set.csv, training_label.csv, test_set.csv, and test_label.csv\n",
    "DIR = 'DatasetAPI/EEG-Motor-Movement-Imagery-Dataset/'\n",
    "SAVE = 'Saved_Files/' + Model + '/'\n",
    "if not os.path.exists(SAVE):  # If the SAVE folder doesn't exist, create one\n",
    "    os.mkdir(SAVE)\n",
    "\n",
    "# Load the dataset, here it uses one-hot representation for labels\n",
    "train_data, train_labels, test_data, test_labels = DatasetLoader(DIR=DIR)\n",
    "train_labels = tf.one_hot(indices=train_labels, depth=4)\n",
    "train_labels = tf.squeeze(train_labels).eval(session=sess)\n",
    "test_labels = tf.one_hot(indices=test_labels, depth=4)\n",
    "test_labels = tf.squeeze(test_labels).eval(session=sess)\n",
    "\n",
    "# Model Hyper-parameters\n",
    "num_epoch = 300   # The number of Epochs that the Model run\n",
    "keep_rate = 0.75  # Keep rate of the Dropout\n",
    "\n",
    "lr = tf.constant(1e-4, dtype=tf.float32)  # Learning rate\n",
    "lr_decay_epoch = 50    # Every (50) epochs, the learning rate decays\n",
    "lr_decay       = 0.50  # Learning rate Decay by (50%)\n",
    "\n",
    "batch_size = 64\n",
    "n_batch = train_data.shape[0] // batch_size\n",
    "\n",
    "# Define Placeholders\n",
    "x = tf.placeholder(tf.float32, [None, 4096])\n",
    "y = tf.placeholder(tf.float32, [None, 4])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Load Model Network\n",
    "prediction = CNN(Input=x, keep_prob=keep_prob)\n",
    "\n",
    "# Load Loss Function\n",
    "loss = loss(y=y, prediction=prediction, l2_norm=True)\n",
    "\n",
    "# Load Optimizer\n",
    "train_step = tf.train.AdamOptimizer(lr).minimize(loss)\n",
    "\n",
    "# Load Evaluation Metrics\n",
    "Global_Average_Accuracy = evaluation(y=y, prediction=prediction)\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "train_writer = tf.summary.FileWriter(SAVE + '/train_Writer', sess.graph)\n",
    "test_writer = tf.summary.FileWriter(SAVE + '/test_Writer')\n",
    "\n",
    "# Initialize all the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "for epoch in range(num_epoch + 1):\n",
    "    # U can use learning rate decay or not\n",
    "    # Here, we set a minimum learning rate\n",
    "    # If u don't want this, u definitely can modify the following lines\n",
    "    learning_rate = sess.run(lr)\n",
    "    if epoch % lr_decay_epoch == 0 and epoch != 0:\n",
    "        if learning_rate <= 1e-6:\n",
    "            lr = lr * 1.0\n",
    "            sess.run(lr)\n",
    "        else:\n",
    "            lr = lr * lr_decay\n",
    "            sess.run(lr)\n",
    "\n",
    "    # Randomly shuffle the training dataset and train the Model\n",
    "    for batch_index in range(n_batch):\n",
    "        random_batch = random.sample(range(train_data.shape[0]), batch_size)\n",
    "        batch_xs = train_data[random_batch]\n",
    "        batch_ys = train_labels[random_batch]\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: keep_rate})\n",
    "\n",
    "    # Show Accuracy and Loss on Training and Test Set\n",
    "    # Here, for training set, we only show the result of first 100 samples\n",
    "    # If u want to show the result on the entire training set, please modify it.\n",
    "    train_accuracy, train_loss = sess.run([Global_Average_Accuracy, loss], feed_dict={x: train_data[0:100], y: train_labels[0:100], keep_prob: 1.0})\n",
    "    Test_summary, test_accuracy, test_loss = sess.run([merged, Global_Average_Accuracy, loss], feed_dict={x: test_data, y: test_labels, keep_prob: 1.0})\n",
    "    test_writer.add_summary(Test_summary, epoch)\n",
    "\n",
    "    # Show the Model Capability\n",
    "    print(\"Iter \" + str(epoch) + \", Testing Accuracy: \" + str(test_accuracy) + \", Training Accuracy: \" + str(train_accuracy))\n",
    "    print(\"Iter \" + str(epoch) + \", Testing Loss: \" + str(test_loss) + \", Training Loss: \" + str(train_loss))\n",
    "    print(\"Learning rate is \", learning_rate)\n",
    "    print('\\n')\n",
    "\n",
    "    # Save the prediction and labels for testing set\n",
    "    # The \"labels_for_test.csv\" is the same as the \"test_label.csv\"\n",
    "    # We will use the files to draw ROC CCurve and AUC\n",
    "    if epoch == num_epoch:\n",
    "        output_prediction = sess.run(prediction, feed_dict={x: test_data, y: test_labels, keep_prob: 1.0})\n",
    "        np.savetxt(SAVE + \"prediction_for_test.csv\", output_prediction, delimiter=\",\")\n",
    "        np.savetxt(SAVE + \"labels_for_test.csv\", test_labels, delimiter=\",\")\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
